<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SpeedCameraPi: Theory</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="speed_64.ico"/></td>
  <td id="projectalign">
   <div id="projectname">SpeedCameraPi<span id="projectnumber">&#160;1.3.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_Docs_markdown_Theory.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Theory </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#autotoc_md169">Image Allignment</a></li>
<li class="level1"><a href="#autotoc_md176">Object Detection</a></li>
<li class="level1"><a href="#autotoc_md181">Speed Calculation</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="autotoc_md169"></a>
Image Allignment</h1>
<h2><a class="anchor" id="autotoc_md170"></a>
Introduction</h2>
<p >Due to various factors, recording a video with a portable handheld device can be challenging. One of the most significant difficulties is keeping the device stable while recording. The device's handheld nature makes it susceptible to movement and shakiness, which can cause a non-static video that significantly affects the accuracy of the calculated results. This section will discuss overcoming this problem by utilizing a feature detection algorithm.</p>
<h2><a class="anchor" id="autotoc_md171"></a>
Scale Invariant Feature Transform (SIFT)</h2>
<p >Many image processing and computer vision algorithms require feature detection in input data, such as object tracking and recognition. Researchers have proposed several algorithms for feature detection, including the Harris corner detector <b>[harris1988]</b>, the SIFT algorithm <b>[lowe2004]</b>, and the SURF algorithm <b>[bay2006]</b>. One significant advantage of Lowe's approach is its ability to detect and describe local features in images insensitive to scale, rotation, and illumination changes. <b>[lowe2004]</b>.</p>
<div class="image">
<img src="sift1.png" alt=""/>
<div class="caption">
Example of SIFT Algorithm</div></div>
 <p >By applying a Difference of Gaussian (DoG) filter to the image at various scales, the SIFT algorithm achieves keypoint localization and generates a scale space representation. Keypoints detection involves identifying local extrema in the scale space, then generating feature descriptors for each key point to facilitate matching and recognition <b>[cordes09]</b>.</p>
<h2><a class="anchor" id="autotoc_md172"></a>
Feature Matching</h2>
<p >Feature matching involves comparing the feature descriptors of the keypoints from the two frames to match the generated keypoints with the keypoints from the next frame. The Fast Library for Approximate Nearest Neighbors (FLANN) is used in this case for comparing feature descriptors that work by comparing the similarity of the generated keypoints and return the best possible match <b>[geo23]</b>.</p>
<p >To further improve the accuracy of the FLANN matcher, Lowe proposed a ratio test to eliminate false matches <b>[lowe2004]</b>. It works by comparing the distance of the best match with the distance of the second-best match, and if the ratio of the distance of the best match to the second-best match is less than a threshold value, the best match is considered to be a good match and is used for further processing <b>[Bekele13]</b>.</p>
<h2><a class="anchor" id="autotoc_md173"></a>
Homography</h2>
<p ><a class="el" href="classHomography.html" title="Helper class for Homography based on the result from Feature Detection.">Homography</a> is a crucial concept in computer vision and is commonly used in perspective transformations <b>[Ondraovi21]</b> and object detection <b>[Hudaya21]</b>. One can use homography in SIFT detection to transform the detected features in one image into the coordinate system of another image, thereby fixing the perspective distortion and aligning the images.</p>
<div class="image">
<img src="homography1.jpg" alt=""/>
<div class="caption">
Example of Homography result \cite cvHomography</div></div>
 <p >As proposed by Vincent and Lagani\'ere, to compute the homography matrix, at least four corresponding points are needed, which can be obtained by the RANSAC algorithm <b>[Vin01]</b>. RANSAC algorithm is an iterative approach that randomly selects a subset of the data and fits a model to the subset <b>[Lee20]</b>. One can fix the perspective distortion of the image and align the images using the computed homography matrix.</p>
<h2><a class="anchor" id="autotoc_md174"></a>
Image Alignment Algorithm</h2>
<div class="image">
<img src="ImageAllignmentAlgoDiagram.jpg" alt=""/>
<div class="caption">
Image Alignment Algorithm</div></div>
 <p >The algorithm starts by reading the selecting two frames from the video and detecting the keypoints using SIFT algorithm. The keypoints are then matched with the keypoints from the next frame using the FLANN matcher. The matched keypoints are utilized to compute the homography matrix using the RANSAC algorithm. The resulting homography matrix is then applied to transform the keypoints from the next frame into the coordinate system of the first frame. This process is repeated for all frames in the video.</p>
<h2><a class="anchor" id="autotoc_md175"></a>
Useful links</h2>
<p >FeatureDetector::allign()</p>
<p ><a href="https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html">OpenCV Example</a></p>
<h1><a class="anchor" id="autotoc_md176"></a>
Object Detection</h1>
<h2><a class="anchor" id="autotoc_md177"></a>
Introduction</h2>
<p >Identifying and locating objects within an image or video is a crucial task in computer vision known as object detection. This task has a wide range of applications, such as in autonomous driving, surveillance, and robotics. Over the years, several methods have been developed to address this problem, ranging from classical computer vision approaches to deep learning-based techniques.</p>
<p >Classical methods include feature-based approaches such as Haar cascades <b>[viola01]</b> and Histogram of Oriented Gradients (HOG) <b>[dalal05]</b>, while deep learning-based approaches include popular frameworks such as Region-based Convolutional Neural Networks (R-CNN) <b>[ross13]</b> and Single Shot MultiBox Detector (SSD) <b>[wei15]</b>. With the recent advancements in computer hardware and deep learning techniques, object detection has seen significant progress, and state-of-the-art models such as YOLO (You Only Look Once) <b>[joseph15]</b> have achieved remarkable accuracy and speed.</p>
<p >Due to the hardware limitations for this project, implementing object detection has been a challenging task to solve. A method which neither requires a powerful computatioanal power nor time consuming is needed. This section will propose a method to detect moving objects in a video with Optical Flow algorithm</p>
<h2><a class="anchor" id="autotoc_md178"></a>
Optical Flow</h2>
<p >Optical flow is a motion estimation technique used to determine the motion of pixels in a video, resulting in a vector field where each vector represents the motion of the corresponding pixel <b>[Duncan92]</b>. One significant advantage of optical flow is its ability to detect independent moving objects in a scene even when limited information is available <b>[Jiang13]</b>.</p>
<p >The technique proposed by Lucas and Kanade estimates pixel displacement by comparing the intensity of pixels in consecutive frames. In order to reduce the complexity of calculating optical flow, two assumptions are made by Lucas and Kanade. These assumptions are that the motion between frames is small and that the intensity of the image is smooth <b>[Sharmin12]</b>.</p>
<div class="image">
<img src="opticalflow_lk.jpg" alt=""/>
<div class="caption">
Result of Optical Flow</div></div>
 <p >Optical flow works by first detecting the keypoints in the first frame of the video as explained in <a class="el" href="classFeatureDetector.html" title="Class for detecting and matching features between two images and alligning them.">FeatureDetector</a>. One study found that feature detection techniques like Canny and Harris can produce more accurate keypoints for optical flow compared to SIFT <b>[Nourani12]</b>. The keypoints are then tracked in the next frame using the Lucas-Kanade algorithm and repeated until all frames are processed.</p>
<p >The result of optical flow, as shown in Figure ... , clearly shows that moving points can be distinctly identified and tracked. Therefore, if a threshold value is set for the magnitude of the vector, fixed points on the background object, e.g., trees, and roads, will be filtered out, and only moving points will be detected.</p>
<h2><a class="anchor" id="autotoc_md179"></a>
Object Detection and Tracking Algorithm</h2>
<div class="image">
<img src="ObjectDetection.png" alt=""/>
<div class="caption">
Object Detection and Tracking Algorithm</div></div>
 <p >The algorithm starts by reading the video and generating initial keypoints using the Shi-Tomasi algorithm. The keypoints are then tracked using the Lucas-Kanade algorithm, and the process is repeated until all frames are processed. The magnitude of the vector is then analyzed. The corresponding point is considered a moving point for any vector whose magnitude is more significant than a threshold value.</p>
<h2><a class="anchor" id="autotoc_md180"></a>
Useful Links</h2>
<p >ObjectDetection</p>
<p >ObjectDetection::updateFlow</p>
<p ><a href="https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html">OpenCV Optical Flow</a></p>
<h1><a class="anchor" id="autotoc_md181"></a>
Speed Calculation</h1>
<h2><a class="anchor" id="autotoc_md182"></a>
Pinhole Camera Model</h2>
<p >The utilization of the pinhole camera model in computer vision and image processing has been widespread. Its origin dates back to the fifth century BC, when Mo Jing, a Chinese Mohist philosopher, described a similar design utilizing a closed room and a hole in the wall <b>[Ye2014]</b>. Later in the tenth century AD, the Arabian physicist and mathematician Ibn al-Haytham, also known as Alhazen, conducted experiments on image formation. He placed three candles in a row, a screen with a small hole between them and the wall, and noticed that images were only formed by the small holes, and the candle to the right created an image on the wall to the left <b>[Bekele13]</b>.</p>
<div class="image">
<img src="pinholemodel.jpg" alt=""/>
<div class="caption">
Example of Pinhole Camera Model \cite Ye2014</div></div>
 <p >The pinhole camera model is fundamental in computer vision, and the concept is often used as a reference for developing more complex camera models. It is a single-point perspective camera model that assumes that each point in the object space projects through a single point in the image plane (Figure pinholemodel}) <b>[Huang14]</b>. The model contains three main components: the image plane, focal length, and optical center <b>[Nogueira19]</b>. The image plane is where the light rays from the object space are projected and where an inverted image is formed <b>[Tomasi15]</b>. The optical center, on the other hand, is where the light rays from the object space intersect, and it is located at a focal length distance from the image plane <b>[Tomasi15]</b>.</p>
<h2><a class="anchor" id="autotoc_md183"></a>
Object Distance from Camera</h2>
<div class="image">
<img src="pinhole2.jpg" alt=""/>
<div class="caption">
Figure of Pinhole Camera Model</div></div>
 <p >The diagram in Figure above shows the formation of an image of an object in the image plane. The object, \(O\), is located at a distance of \(O_d\) from the optical center, \(C\) where the light rays from the object space intersect. It will then form an inverted image of the object at the image plane, \(I\), which located at a distance of \(f\) from the optical center. \(O_h\) represents the height of the object in real world and \(I_h\) represents the height of the object in image plane. \(S_h\) is the height of the image plane or the sensor size of the camera.</p>
<p >The mathematical equations for the pinhole camera model are derived based on the assumptions that the projection of the object is a point <b>[Huang14]</b> and that light propagates in a straight line <b>[bhathal09]</b>. From these assumptions, we can derive the following equations:</p>
<p >\begin{equation} \label{eq:1} \frac{O_h}{O_d} = \frac{I_h}{f} \end{equation}</p>
<p >\begin{equation} \label{eq:2} \frac{S_h}{I_h} = \frac{S_{hpix}}{I_{hpix}} \end{equation}</p>
<p >Assuming that the camera utilizes 100% of the sensor size <b>[picamDoc]</b>, the following equation can be derived to relate the sensor size and image resolution, as stated in Equation eq:2} . This equation assumes that the ratio of sensor size, \(S_h\), to focal length, \(f \), remains constant in comparison to the ratio of image resolution, \(S_{hpix}\), and object size in pixels at the image plane, \(I_{hpix}\) . By using this relationship, the value of \(I_h\) in Equation eq:1} can be substituted to calculate the distance of the object from the camera using the following equation:</p>
<p >\begin{equation} \label{eq:3} O_d = {f} \cdot \frac{S_{hpix}}{S_h} \cdot \frac{O_h}{I_{hpix}} \end{equation}</p>
<p >One can obtain the parameters \(S_{hpix}\) and \(f\) directly from the camera specifications or manufacturer. For instance, the Raspberry Pi Camera Module v2 used in this project has a sensor size of 3.68 mm x 2.76 mm, and a focal length of 3.04 mm <b>[pi]</b>. It is worth noting that the sensor size varies in width and height, and the choice of which value to use in the calculation depends on the camera's orientation concerning the object's position.</p>
<p >While \(S_{hpix}\) depends on the camera resolution, \(I_{hpix}\) value can be measured directly from the image. However, we are left with an unknown variable, \(O_h\), representing the object's height in the real world. Hence, to compute the object's distance from the camera, one must know the object's height in the real world.</p>
<h2><a class="anchor" id="autotoc_md184"></a>
Speed Calculation</h2>
<p >To determine the speed of a moving object, one can calculate the distance it travels within a given time interval. This can be done by subtracting the distance of the object in the previous frame from the distance of the object in the current frame. Similarly, the time interval can be calculated by subtracting the time of the previous frame from the current frame's time. Therefore, the formula for determining the speed of the object is derived.</p>
<p >\begin{equation} \label{eq:4} Speed = \frac{|d_{current} - d_{previous}|}{t_{current} - t_{previous}} \end{equation}</p>
<p >However, as previously mentioned in {objectdistance}, the object's dimension in the real world must be known to obtain an accurate result in the calculation. A constant object size in the real world must be utilized to ensure consistency in the calculation. In this project, the width of the road, which has an average value of 3.5 meters in Germany, will be utilized as the object size <b>[Autobahn]</b>.</p>
<h2><a class="anchor" id="autotoc_md185"></a>
Proposed Algorithm</h2>
<div class="image">
<img src="algo.jpg" alt=""/>
<div class="caption">
Proposed Algorithm</div></div>
 <p >Figure {algorithm} illustrates the proposed algorithm, which commences by acquiring images from the camera. Next, the image alignment algorithm processes the images to eliminate any perspective distortion and ensure proper alignment. The object detection algorithm then employs optical flow to detect moving objects in the image.</p>
<p >User input is required to select the line that represents the width of the road, as the calculation requires the real-world size of the object, as explained in {objectdistance}. Once the line selection is made, the speed of the detected object can be calculated. The calculation results are displayed on the screen, and the process is terminated. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
